{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First generate ds1_stats running python script generate_stats_for_ds_1.py \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "#param\n",
    "\n",
    "if len(sys.argv)<3:\n",
    "    print(\"Enter valid query folder as argument\")\n",
    "    sys.exit()\n",
    "elif len(sys.argv)>3:\n",
    "    print(\"Invalid Arguments\")\n",
    "    sys.exit()\n",
    "elif sys.argv[2] != '-ds1' or sys.argv[2] != '-ds2':\n",
    "    print('Enter valid flag -ds1 , -ds2 ')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# query_folder = \"query_data/\"\n",
    "query_folder = sys.argv[1]\n",
    "# metric_flag = '-ds1'\n",
    "metric_flag = sys.argv[2]\n",
    "\n",
    "\n",
    "if metric_flag == '-ds1':\n",
    "    if not os.path.isfile('Results/ds1_stats.csv') :\n",
    "        print(' First generate ds1_stats running python script generate_stats_for_ds_1.py ')\n",
    "        sys.exit()\n",
    "if metric_flag == '-ds2':\n",
    "    if not os.path.isfile('Results/ds2_stats.csv') :\n",
    "        print(' First generate ds2_stats running python script generate_stats_for_ds_2.py ')\n",
    "        sys.exit()\n",
    "onlyfiles = [f for f in listdir(query_folder) if isfile(join(query_folder, f)) and \"_bme\" in f]\n",
    "# print(onlyfiles)\n",
    "dates = []\n",
    "for f in onlyfiles:\n",
    "    date = f.split(\"_\")[0]\n",
    "    dates.append(date)\n",
    "# print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "\n",
    "import seaborn as sns\n",
    "from dateutil import tz\n",
    "import pytz \n",
    "import tqdm\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "import folium\n",
    "from branca.element import Figure\n",
    "from folium.plugins import HeatMapWithTime, HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# reading and preprocessing related functions\n",
    "def read_raw(folder,date):\n",
    "    \"\"\"\n",
    "    Reads in the data. Does not remove any row. \n",
    "    \"\"\"\n",
    "#     if len(str(date[8:])) == 2:\n",
    "    df_bme = pd.read_csv(folder + \"/\" + date + \"_bme.csv\", index_col= 0)\n",
    "    df_gps = pd.read_csv(folder + \"/\" + date + \"_gps.csv\", index_col = 0)\n",
    "    df_pol = pd.read_csv(folder + \"/\" + date + \"_pol.csv\", index_col = 0)\n",
    "#     else:\n",
    "#         df_bme = pd.read_csv(\"data/\" + date + \"_bme.csv\", index_col= 0)\n",
    "#         df_gps = pd.read_csv(\"data/\" + date + \"_gps.csv\", index_col = 0)\n",
    "#         df_pol = pd.read_csv(\"data/\" + date + \"_pol.csv\", index_col = 0)\n",
    "    return df_bme, df_gps, df_pol\n",
    "\n",
    "def handle_dateTime(df_all):\n",
    "    ## change dateTime column from type \"object\" to \"datetime\"\n",
    "    df_all[\"dateTime\"] = pd.to_datetime(df_all.dateTime)    \n",
    "    # convert to India timing\n",
    "    to_zone = tz.gettz('Asia/Kolkata')\n",
    "    df_all.dateTime = df_all.dateTime.apply(lambda x: pytz.utc.localize(x, is_dst=None).astimezone(to_zone))\n",
    "    return df_all\n",
    "\n",
    "def make_time_cols(df_all):\n",
    "    df_all[\"hour\"] = df_all.dateTime.dt.hour\n",
    "    df_all[\"minute\"] = df_all.dateTime.dt.minute    \n",
    "    return df_all\n",
    "\n",
    "def preprocess(df_tuple):\n",
    "    \"\"\"\n",
    "    Combines all other functions\n",
    "    \"\"\"\n",
    "    df_bme, df_gps, df_pol = df_tuple\n",
    "    \n",
    "    # drop duplicates\n",
    "    df_bme = df_bme.drop_duplicates(subset =\"uid\" )\n",
    "    df_gps = df_gps.drop_duplicates(subset = \"uid\")\n",
    "    df_pol = df_pol.drop_duplicates(subset = \"uid\")\n",
    "    \n",
    "    # merge on key columns\n",
    "    key_cols = [\"uid\", \"dateTime\", \"deviceId\"]\n",
    "    df_all = pd.merge(df_bme, df_gps, on = key_cols)\n",
    "    df_all = pd.merge(df_all, df_pol , on = key_cols)\n",
    "    \n",
    "    # rename lng to long and shorten device IDs\n",
    "    df_all = df_all.rename(columns = {\"lng\":\"long\"})\n",
    "    df_all.deviceId = df_all.deviceId.str[-5:]\n",
    "    \n",
    "    # handle dateTime and time related columns\n",
    "    df_all = handle_dateTime(df_all)\n",
    "    df_all = make_time_cols(df_all)\n",
    "    \n",
    "    # some final stuff \n",
    "    df_all = df_all.sort_values(\"dateTime\")\n",
    "    df_all = df_all.reset_index(drop = True)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def plot_plotwise_for_all_dates(dates):\n",
    "    result = []\n",
    "    counter = 1\n",
    "    print('Generating ds_1 stats for dates',dates)\n",
    "    print('Progress ',0,'of',len(dates))\n",
    "    for dt in dates :\n",
    "        df_bme, df_gps, df_pol = read_raw(query_folder,dt)\n",
    "        df_all = preprocess((df_bme, df_gps, df_pol))\n",
    "        sensor_order = df_all.deviceId.unique()\n",
    "        sensor_order.sort()\n",
    "        \n",
    "        sub = df_all.groupby([\"deviceId\", \"hour\", \"minute\"]).size().reset_index()\n",
    "#         fig, ax = plt.subplots(1, 1, figsize = (15, 5))\n",
    "#             print(sub.describe())\n",
    "#         print(sub.groupby([\"deviceId\"])[0].describe())\n",
    "        result.append(sub.groupby([\"deviceId\"])[0].describe())\n",
    "#         g = sns.boxplot(y = 0, data = sub, x= \"deviceId\", order = sensor_order)\n",
    "#         g.set_ylabel(\"Samples Recorded per Minute\")\n",
    "#         g.set_title(dt+ \": Boxplots for Sampling Rate (Samples Recorded per Minute)\")\n",
    "\n",
    "#         plt.show()\n",
    "        \n",
    "        print('Progress ',counter,'of',len(dates))\n",
    "        counter +=1\n",
    "        \n",
    "    return result\n",
    "\n",
    "def plot_plotwise_for_all_dates_ds2(dates):\n",
    "    result = []\n",
    "    counter = 1\n",
    "    print('Generating ds_2 stats for dates',dates)\n",
    "    print('Progress ',0,'of',len(dates))\n",
    "    for dt in dates :\n",
    "        df_bme, df_gps, df_pol = read_raw(query_folder,dt)\n",
    "        df_all = preprocess((df_bme, df_gps, df_pol))\n",
    "        sensor_order = df_all.deviceId.unique()\n",
    "        sensor_order.sort()\n",
    "    \n",
    "        sub = df_all.groupby([\"deviceId\", \"hour\", \"minute\"]).size().reset_index()\n",
    "        sub_2 = sub.groupby([\"deviceId\", \"hour\"]).size().reset_index()\n",
    "#         print(sub_2.groupby([\"deviceId\"])[0].describe())\n",
    "        result.append(sub_2.groupby([\"deviceId\"])[0].describe())\n",
    "#         fig, ax = plt.subplots(1, 1, figsize = (15,5))\n",
    "#         g = sns.boxplot(y = 0, data = sub_2, x= \"deviceId\", order = sensor_order)\n",
    "#         g.set_ylabel(\"Number of Minutes Active in an Hour\")\n",
    "#         g.set_title(dt+ \": Boxplots for Number of Minutes a Device is Active in an Hour\")\n",
    "#         plt.show()\n",
    "        print('Progress ',counter,'of',len(dates))\n",
    "        counter +=1\n",
    "    return result\n",
    "\n",
    "def detect_anomaly(dates,metric_flag):\n",
    "    if metric_flag == '-ds1':\n",
    "        ds_stats = pd.read_csv('Results/ds1_stats.csv',index_col='metric')\n",
    "        stats = ds_stats.to_dict()\n",
    "    if metric_flag == '-ds2':\n",
    "        ds_stats = pd.read_csv('Results/ds2_stats.csv',index_col='metric')\n",
    "        stats = ds_stats.to_dict()\n",
    "    \n",
    "    print('Finding anomaly based of below stats : ')\n",
    "    print(ds_stats)\n",
    "    print('-----------------------------------------------------')\n",
    "    # ds1_stats['lower_limit']['median']\n",
    "    lower_median = stats['lower_limit']['25th_percentile']\n",
    "    upper_median = stats['upper_limit']['75th_percentile']\n",
    "    lower_25th = stats['lower_limit']['25th_percentile']\n",
    "    upper_25th = stats['upper_limit']['75th_percentile']\n",
    "    lower_75th = stats['lower_limit']['25th_percentile']\n",
    "    upper_75th = stats['upper_limit']['75th_percentile']\n",
    "\n",
    "    # print(dates)\n",
    "\n",
    "\n",
    "    # merge with upper cell\n",
    "    anamolus_flag = False\n",
    "    # print(len(query_result))\n",
    "    for idx,date in enumerate(query_result):\n",
    "        for device in range(len(date)):\n",
    "            anomalous_score = 0\n",
    "            device_median = date.iloc[device]['50%']\n",
    "            device_25th = date.iloc[device]['25%']\n",
    "            device_75th = date.iloc[device]['75%']\n",
    "            if lower_median> device_median or device_median > upper_median:\n",
    "                anomalous_score +=1\n",
    "            if lower_25th> device_25th or device_25th > upper_25th:\n",
    "                anomalous_score +=1\n",
    "            if lower_75th> device_75th or device_75th > upper_75th:\n",
    "                anomalous_score +=1\n",
    "\n",
    "            # anamolus detection logic\n",
    "            if anomalous_score >= 2 : \n",
    "                deviceName = query_result[0].index.values[device]\n",
    "                print('deviceID : ',deviceName,'is behaving anamolusly on date:',dates[idx])\n",
    "                anamolus_flag = True\n",
    "    #             result = print_stats_for_date(dt)\n",
    "                print('Below are stats for date',dates[idx])\n",
    "                print(query_result[idx])\n",
    "\n",
    "    if not anamolus_flag:\n",
    "        print('No anamolus device  on dates',dates,'metric flag:',metric_flag)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ds_2 stats for dates ['2020-12-26']\n",
      "Progress  0 of 1\n",
      "Progress  1 of 1\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"query_data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) and \"_bme\" in f]\n",
    "# print(onlyfiles)\n",
    "dates = []\n",
    "for f in onlyfiles:\n",
    "    date = f.split(\"_\")[0]\n",
    "    dates.append(date)\n",
    "    \n",
    "# metric_flag = '-ds2'\n",
    "if metric_flag == '-ds1':\n",
    "    query_result = plot_plotwise_for_all_dates(dates)\n",
    "elif metric_flag == '-ds2':\n",
    "    query_result = plot_plotwise_for_all_dates_ds2(dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding anomaly based of below stats : \n",
      "                 lower_limit  upper_limit\n",
      "metric                                   \n",
      "median                  42.5        55.50\n",
      "25th_percentile         23.5        32.75\n",
      "75th_percentile         57.5        60.00\n",
      "-----------------------------------------------------\n",
      "No anamolus device  on dates ['2020-12-26'] metric flag: -ds2\n"
     ]
    }
   ],
   "source": [
    "detect_anomaly(dates,metric_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(query_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(query_result[0].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result[0].index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
